# -*- coding: utf-8 -*-
"""Rekomendasi Buku_Cindy Maharani.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dlO1dpDhPp3Oop3fwUcHmHkaTwZdtCuS

# ðŸ“š Sistem Rekomendasi Buku
Sitem rekomendasi saat ini sedang menjadi tren. Saat ini berbelanja online merupakan pilihan yang lebih disukai masyarakat karena kemudahannya dan kecepatannya (Mathew, Praveena  et al., 2016). Pilihan barang - barang yang tersedia online juga semakin banyak, salah satunya adalah buku. Akan tetapi, pilihan yang beragam terkadang membuat konsumen kesulitan untuk memilik buku sesuai dengan preferensi mereka, baik buku secara fisik maupun buku elektronik (e-book). Mereka dapat membaca sinopisis yang tersedia atau sample halaman yang ada pada buku elektronik, tetapi hal tersebut akan memakan banyak waktu jika terdapat banyak buku yang dipertimbangkan. Oleh karena itu, dibutuhkan sistem rekomendasi buku yang membantu pengguna dalam memilih buku yang sesuai dengan preferensi mereka.

Selain untuk membantu pengguna, hal ini juga memberikan keuntungan untuk situs jual beli online yang menjual buku baik. Dengan merekomendasikan buku - buku yang kemungkinan akan dibeli oleh pengguna maka pendapatan mereka akan bertambah.

Sumber: P. Mathew, B. Kuriakose and V. Hegde, "Book Recommendation System through content based and collaborative filtering method," 2016 International Conference on Data Mining and Advanced Computing (SAPIENCE), Ernakulam, India, 2016, pp. 47-52, doi: 10.1109/SAPIENCE.2016.7684166.

Dataset: [Book Recommendation Dataset](https://www.kaggle.com/datasets/arashnic/book-recommendation-dataset)

# Data Understanding

## Import Libraries
"""

import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from tensorflow.keras.callbacks import EarlyStopping
from zipfile import ZipFile
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive')

# Membaca data dari google drive dan menyimpan pada dataframe
books = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Laskar Ai/Submisi Sistem Rekomendasi/dataset/Books.csv')
users = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Laskar Ai/Submisi Sistem Rekomendasi/dataset/Users.csv')
ratings = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Laskar Ai/Submisi Sistem Rekomendasi/dataset/Ratings.csv')

print('Jumlah data buku: ', len(books.ISBN.unique()))
print('Jumlah data user: ', len(users['User-ID'].unique()))
print('Jumlah data rating: ', len(ratings['User-ID']))
print('Jumlah data user yang memberi rating: ', len(ratings['User-ID'].unique()))

"""Terdapat 271.360 Judul buku, 278.858 pengguna, dan 1.149.780 rating dari 105.283 pengguna.

## Univariate Exploratory Data Analysis

### Books Variable
"""

books.info()

"""Terdapat data null pada kolom Book-Author, Publisher, dan Image-URL-L"""

# Menampilkan baris yang memiliki nilai null pada kolom book-author dan publisher
print(books[books['Book-Author'].isnull()])
print(books[books['Publisher'].isnull()])

"""Terdapat 2 baris yang tidak memiliki pengarang dan 2 baris yang tidak memiliki penerbit"""

# Menampilkan jumlah pengarang, jumlah penerbit, dan tahun terbit
print('Jumlah Pengarang: ', len(books['Book-Author'].unique()))
print('Jumlah Penerbit: ', len(books['Publisher'].unique()))
print('Tahun Terbit', books['Year-Of-Publication'].unique())

"""Dapat dilihat bahwa pada data tahun masih belum seragam, ada yang menggunakan tanda petik dan ada yang tidak. Selain itu, terdapat data yang salah, yaitu 'DK Publishing Inc' dan 'Gallimard' yang seharusnya berada pada kolom Publisher (penerbit)."""

# Mengatur opsi tampilan Pandas agar tidak memotong string
pd.set_option('display.max_colwidth', None)

# Melihat data yang bernilai salah pada kolom year = 'DK Publishing Inc"
books.loc[books['Year-Of-Publication'] == 'DK Publishing Inc',:]

# Melihat data yang bernilai salah pada kolom year = 'Gallimard"
books.loc[books['Year-Of-Publication'] == 'Gallimard',:]

"""Dapat dilihat bahwa kolom Year-Of-Publication, Book-Author, dan Publisher memiliki misfielded value karena nilainya tergeser akibat Book-Author yang nilainya bergabung dengan Book-Title."""

books.describe(include='all')

"""Terdapat data judul buku sebesar 271.360 tetapi yang memiliki judul berbeda (unik) ada 102.022. Hal ini menunjukkan bahwa ada buku yang memiliki judul yang sama."""

duplicate_titles = books[books.duplicated(subset=['Book-Title'], keep=False)]['Book-Title'].value_counts()
print(duplicate_titles)

"""Buku - buku yang memiliki judul dan pengarang yang sama tetapi memiliki ISBN yang berbeda mungkin saja terjadi jika penerbitnya berbeda atau karena adanya revisi atau edisi baru dengan perubahan konten atau format. Jadi hal ini adalah hal yang wajar, bukan suatu kesalahan.

### Users Variable
"""

users.info()

"""Dapat dilihat bahwa terdapat banyak missing value pada kolom Age"""

users.describe(include='all')

# Create a box plot
plt.figure(figsize=(8, 6))
plt.xlim([0, 130])
users.Age.hist(bins=40)

"""Age memiliki distribusi 'Right Skewed', distribusi data yang tidak simetris, dengan ekor yang lebih panjang di sisi kanan daripada sisi kiri. Pada distribusi Age di atas, terlihat ada data umur yang tidak masuk akal untuk mereview buku, yaitu di bawah 5 tahun dan di atas 90 tahun.

### Ratings Variable
"""

ratings.info()

ratings.head()

pd.options.display.float_format = '{:.0f}'.format
ratings.describe(include='all')

"""Berdasarkan informasi dataset pada sumbernya, kaggle, diketahui bahwa Penilaian buku (Book-Rating) memiliki dua jenis:

- Eksplisit: Penilaian ini diberikan secara langsung oleh pengguna dalam bentuk angka pada skala 1 hingga 10. Semakin tinggi angka yang diberikan, semakin tinggi apresiasi atau kesukaan pengguna terhadap buku tersebut.
- Implisit: Penilaian ini tidak diberikan secara langsung, melainkan ditandai dengan angka 0. Angka 0 ini mengindikasikan bahwa pengguna mungkin telah berinteraksi dengan buku tersebut (misalnya, melihat detailnya, menambahkannya ke daftar bacaan, atau melakukan tindakan lain), tetapi tidak memberikan penilaian eksplisit dalam skala 1-10.

Saya akan fokus membangun model collaborative filtering pada dara rating eksplisit, sehingga data rating implisit nantinya akan dihapus.
"""

# Menghitung data rating implisit
ratings.loc[ratings['Book-Rating'] == 0].count()

"""# Data Preparation

## Data Cleaning
"""

# Menghapus kolom Image URL karena tidak digunakan pada analisis
books.drop(['Image-URL-S', 'Image-URL-M', 'Image-URL-L'], axis=1, inplace=True)

# Mengisi nilai yang kosong pada kolom 'Book-Author' dan 'Publisher' dengan 'other'
books['Book-Author'] = books['Book-Author'].fillna('other')
books['Publisher'] = books['Publisher'].fillna('other')

# Cek apakah data telah berhasil diubah menjadi 'other'
books.loc[books['Book-Author'] == 'other', :]

# Memperpaiki nilai yang tergeser
books.loc[books['ISBN'] == '078946697X', 'Book-Title'] = 'DK Readers: Creating the X-Men, How It All Began (Level 4: Proficient Readers)'
books.loc[books['ISBN'] == '078946697X', 'Book-Author'] = 'Michael Teitelbaum'
books.loc[books['ISBN'] == '078946697X', 'Year-Of-Publication'] = 2000
books.loc[books['ISBN'] == '078946697X', 'Publisher'] = 'DK Publishing Inc'

books.loc[books['ISBN'] == '0789466953', 'Book-Title'] = 'DK Readers: Creating the X-Men, How Comic Books Come to Life (Level 4: Proficient Readers)'
books.loc[books['ISBN'] == '0789466953', 'Book-Author'] = 'James Buckley'
books.loc[books['ISBN'] == '0789466953', 'Year-Of-Publication'] = 2000
books.loc[books['ISBN'] == '0789466953', 'Publisher'] = 'DK Publishing Inc'

books.loc[books['ISBN'] == '2070426769', 'Book-Title'] = "Peuple du ciel, suivi de 'Les Bergers"
books.loc[books['ISBN'] == '2070426769', 'Book-Author'] = 'Jean-Marie Gustave Le ClÃƒ?Ã‚Â©zio'
books.loc[books['ISBN'] == '2070426769', 'Year-Of-Publication'] = 2003
books.loc[books['ISBN'] == '2070426769', 'Publisher'] = 'Gallimard'

# Cek salah satu baris
books.loc[books['ISBN'] == '078946697X', :]

# Konversi Year-Of-Publication ke string
books['Year-Of-Publication'] = books['Year-Of-Publication'].astype(str)
# Ubah ke nilai NaN untuk tahun yang tidak valid, yaitu di atas 2025 dan dibawah 1000
books['Year-Of-Publication'] = books['Year-Of-Publication'].apply(lambda x: x if x.isdigit() and 1000 <= int(x) <= 2025 else np.nan)
# Konversi ke integer
books['Year-Of-Publication'] = books['Year-Of-Publication'].astype(float).astype('Int64')
# Imputasi Nilai Kosong dengan nilai median
books['Year-Of-Publication'] = books['Year-Of-Publication'].fillna(books['Year-Of-Publication'].median())

print(np.sort(books['Year-Of-Publication'].unique()))

books.info()

# Mengbah umur yang tidak masuk akal menjadi nan
users.loc[(users.Age > 90) | (users.Age < 5), 'Age'] = np.nan

# Gunakan random imputation pada nilai NaN, nilai random antar rentang 20-60
users.loc[users['Age'].isnull(), 'Age'] = np.random.randint(20, 61, size=users['Age'].isnull().sum())

# Pastikan hasilnya integer
users['Age'] = users['Age'].astype(int)

# Create a box plot
plt.figure(figsize=(8, 6))
users.Age.hist(bins=40)

# Ambil hanya data rating eksplisit
ratings_explicit = ratings[ratings['Book-Rating'] != 0]

print(f"Jumlah baris sebelum drop implisit: {len(ratings)}")
print(f"Jumlah baris setelah drop implisit: {len(ratings_explicit)}")

ratings = ratings_explicit

"""## Mengetahui Jumlah Rating"""

# Menggabungkan dataframe ratings dengan books berdasarkan nilai User-ID
book_rating = pd.merge(ratings, books , on='ISBN', how='inner')
book_rating

# Filter data karena keterbatasan RAM
isbn_counts = book_rating['ISBN'].value_counts()

filtered_book_rating = book_rating[book_rating['ISBN'].isin(isbn_counts[isbn_counts >= 50].index)]

filtered_book_rating.info()

# Atur style seaborn
sns.set(style="whitegrid")

# Plot distribusi rating
plt.figure(figsize=(8, 5))
sns.countplot(data=filtered_book_rating, x='Book-Rating', hue = 'Book-Rating')

plt.title('Distribusi Rating Buku')
plt.xlabel('Rating')
plt.ylabel('Jumlah')
plt.xticks(rotation=0)
plt.tight_layout()
plt.show()

"""Rating terbanyak yang diberikan adalah 8/10. Diikuti dengan 10, 9, 7, dan seterusnya menurun."""

# Membuat variabel preparation yang berisi dataframe book_rating kemudian mengurutkan berdasarkan User-ID
preparation = filtered_book_rating
preparation.sort_values('ISBN')

# Membuang data duplikat pada variabel preparation
preparation = preparation.drop_duplicates('ISBN')
preparation

# Visualisasi total buku dan tahun pada filtered_book_rating
plt.figure(figsize=(12, 6))
sns.countplot(x='Year-Of-Publication', data=preparation)
plt.title('Jumlah Terbitan Buku per Tahun')
plt.xlabel('Tahun Publikasi')
plt.ylabel('Jumlah Buku')
plt.xticks(rotation=45, ha='right') # Rotasi label sumbu x agar lebih mudah dibaca
plt.tight_layout()
plt.show()

"""Distribusi jumlah buku per tahun menunjukkan kurva left skewed dimana pada tahun - tahun lama jumlah buku sedikit tetapi di tahun yang lebih baru memiliki jumlah buku yang banyak terutama tahun 2002."""

# Mengonversi data series 'ISBN' menjadi dalam bentuk list
book_id = preparation['ISBN'].tolist()

# Mengonversi data series 'Book-Title' menjadi dalam bentuk list
title = preparation['Book-Title'].tolist()

# Mengonversi data series 'Book-Author' menjadi dalam bentuk list
author = preparation['Book-Author'].tolist()

# Mengonversi data series 'Publisher' menjadi dalam bentuk list
publisher = preparation['Publisher'].tolist()

print(len(book_id))
print(len(title))
print(len(author))
print(len(publisher))

# Membuat dictionary untuk data 'book_id', 'title', 'author', dan 'publisher'
book_new = pd.DataFrame({
    'id': book_id,
    'book_title': title,
    'book_author': author,
    'publisher': publisher
})
book_new

"""# Model Development

## Content Based Filtering
"""

data = book_new
data.sample(5)

data['features'] = (
    data['book_title'].fillna('') + ' ' +
    data['book_author'].fillna('') + ' ' +
    data['publisher'].fillna('')
)

"""### TF-IDF Vectorizer"""

# Inisialisasi TfidfVectorizer
tfidf = TfidfVectorizer(stop_words='english')

# Melakukan perhitungan idf pada data features
tfidf.fit(data['features'])

# Mapping array dari fitur index integer ke fitur nama
tfidf.get_feature_names_out()

# Melakukan fit lalu ditransformasikan ke bentuk matrix
tfidf_matrix = tfidf.fit_transform(data['features'])

# Melihat ukuran matrix tfidf
tfidf_matrix.shape

# Mengubah vektor tf-idf dalam bentuk matriks dengan fungsi todense()
tfidf_matrix.todense()

"""### Cosine Similarity"""

# Menghitung cosine similarity pada matrix tf-idf
cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

# Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa judul buku
cosine_sim_df = pd.DataFrame(cosine_sim, index=data['book_title'], columns=data['book_title'])
print('Shape:', cosine_sim_df.shape)

# Melihat similarity matrix pada setiap buku
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""### Inference"""

# Fungsi untuk merekomendasikan buku
def book_recommendations(title, similarity_data=cosine_sim_df, items=data[['id', 'book_title', 'book_author', 'publisher']], k=5):
    index = similarity_data.loc[:, title].to_numpy().argpartition(range(-1, -k, -1))
    closest = similarity_data.columns[index[-1:-(k+2):-1]]
    closest = closest.drop(title, errors='ignore')
    return pd.DataFrame(closest).merge(items).head(k)

data[data.book_title.eq('The Girl Who Loved Tom Gordon : A Novel')]

book_recommendations('The Girl Who Loved Tom Gordon : A Novel')

"""### Evaluasi

Dari inference ini, kita dapat mengukur presisi dari sistem rekomendasi buku menggunakan content based filtering ini menggunakan rumus:

$$
P = \frac{\text{of our recommendations that are relevant}}{\text{of items we recommended}}
$$

Dimana:
- $P$ adalah nilai Precision.
- *# of our recommendations that are relevant* adalah jumlah rekomendasi yang diberikan oleh sistem yang relevan bagi pengguna.
- *# of items we recommended* adalah jumlah total item yang direkomendasikan oleh sistem.

Dapat dilihat bahwa 5 buku yang direkomendasikan memiliki kesamaan dengan judul, penulis, dan penerbit,

Maka:

$$
\frac{\text{5 item relevan}}{\text{5 item yang direkomendasikan}} = 1
$$

## Collaborative Filtering
"""

# Membaca Dataset
df = filtered_book_rating[['User-ID', 'ISBN', 'Book-Rating']].copy()
df = df.rename(columns={'User-ID': 'user_id', 'ISBN': 'isbn', 'Book-Rating': 'rating'})
df

# Mengubah user_id menjadi list tanpa nilai yang sama
user_ids = df['user_id'].unique().tolist()
print('list user_id: ', user_ids)

# Melakukan encoding user_id
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded user_id : ', user_to_user_encoded)

# Melakukan proses encoding angka ke user_id
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke user_id: ', user_encoded_to_user)

# Mengubah isbn menjadi list tanpa nilai yang sama
book_ids = df['isbn'].unique().tolist()

# Melakukan proses encoding isbn
book_to_book_encoded = {x: i for i, x in enumerate(book_ids)}

# Melakukan proses encoding angka ke isbn
book_encoded_to_book = {i: x for i, x in enumerate(book_ids)}

# Mapping user_id ke dataframe user
df['user'] = df['user_id'].map(user_to_user_encoded)

# Mapping isbn ke dataframe book
df['book'] = df['isbn'].map(book_to_book_encoded)

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(num_users)

# Mendapatkan jumlah buku
num_books = len(book_encoded_to_book)
print(num_books)

# Mengubah rating menjadi nilai float
df['rating'] = df['rating'].values.astype(np.float32)

# Nilai minimum rating
min_rating = min(df['rating'])

# Nilai maksimal rating
max_rating = max(df['rating'])

print('Number of User: {}, Number of Book: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_books, min_rating, max_rating
))

"""### Splitting Data"""

# Mengacak dataset
df = df.sample(frac=1, random_state=42)
df

# Membuat variabel x untuk mencocokkan data user dan book menjadi satu value
x = df[['user', 'book']].values

# Membuat variabel y untuk membuat rating dari hasil
y = df['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""### Training"""

class RecommenderNet(tf.keras.Model):

  # Insialisasi fungsi
  def __init__(self, num_users, num_book, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_book = num_book
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.book_embedding = layers.Embedding( # layer embeddings book
        num_book,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.book_bias = layers.Embedding(num_book, 1) # layer embedding book bias

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    book_vector = self.book_embedding(inputs[:, 1]) # memanggil layer embedding 3
    book_bias = self.book_bias(inputs[:, 1]) # memanggil layer embedding 4

    dot_user_book = tf.tensordot(user_vector, book_vector, 2)

    x = dot_user_book + user_bias + book_bias

    return tf.nn.sigmoid(x)

model = RecommenderNet(num_users, num_books, 20) # inisialisasi model

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

early_stop = EarlyStopping(
    monitor='val_root_mean_squared_error',
    patience=5,
    restore_best_weights=True
)

# Memulai training

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 32,
    epochs = 100,
    validation_data = (x_val, y_val),
    callbacks=[early_stop]
)

"""### Evaluasi dan Visualisasi Metrik"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""Proses training model cukup smooth dan model konvergen pada epochs sekitar 29. Dari proses ini, kita memperoleh nilai error akhir sebesar sekitar 0.13 dan error pada data validasi sebesar 0.19. Nilai tersebut cukup bagus untuk sistem rekomendasi."""

def precision_recall_at_k(model, x_val, y_val, k=10):
    # Asumsikan x_val bentuknya [user_id, book_id], dan y_val adalah rating sebenarnya
    hit = 0
    total_recommended = 0
    total_relevant = 0

    user_ids = np.unique(x_val[:, 0])

    for user_id in user_ids:
        # Ambil semua data val user ini
        idx = x_val[:, 0] == user_id
        books = x_val[idx, 1]
        ratings_true = y_val[idx]

        if len(books) < k:
            continue

        # Prediksi untuk semua buku milik user ini
        input_pairs = np.hstack((np.full((len(books), 1), user_id), books.reshape(-1, 1)))
        preds = model.predict(input_pairs).flatten()

        # Ambil Top-K
        top_k_idx = preds.argsort()[-k:][::-1]
        recommended_ratings = ratings_true[top_k_idx]

        # Asumsikan rating > 0.5 adalah relevan
        hit += np.sum(recommended_ratings >= 0.5)
        total_recommended += k
        total_relevant += np.sum(ratings_true >= 0.5)

    precision = hit / total_recommended
    recall = hit / total_relevant
    return precision, recall

precision, recall = precision_recall_at_k(model, x_val, y_val, k=10)
print(f"Precision@10: {precision:.4f}")
print(f"Recall@10: {recall:.4f}")

"""Hasil:

- Precision@10: 0.8389 â€” 84% dari top-10 rekomendasi sesuai preferensi pengguna.

- Recall@10: 0.6089 â€” sistem berhasil merekomendasikan lebih dari 60% buku yang relevan.

### Inference
"""

book_df = book_new
df = filtered_book_rating[['User-ID', 'ISBN', 'Book-Rating']].copy()

# Mengambil sample user
user_id = df['User-ID'].sample(1).iloc[0]
book_rated_by_user = df[df['User-ID'] == user_id]

# Operator bitwise (~)
book_not_rated = book_df[~book_df['id'].isin(book_rated_by_user['ISBN'].values)]['id']
book_not_rated = list(
    set(book_not_rated)
    .intersection(set(book_to_book_encoded.keys()))
)

book_not_rated = [[book_to_book_encoded.get(x)] for x in book_not_rated]
user_encoder = user_to_user_encoded.get(user_id)
user_book_array = np.hstack(
    ([[user_encoder]] * len(book_not_rated), book_not_rated)
)

ratings = model.predict(user_book_array).flatten()

top_ratings_features = ratings.argsort()[-10:][::-1]
recommended_book_ids = [
    book_encoded_to_book.get(book_not_rated[x][0]) for x in top_ratings_features
]

print('Showing recommendations for users: {}'.format(user_id))
print('====' * 8)
print('Book with high ratings from user')
print('----' * 8)

top_book_user = (
    book_rated_by_user.sort_values(
        by = 'Book-Rating',
        ascending=False
    )
    .head(5)
    .ISBN.values
)

book_df_rows = book_df[book_df['id'].isin(top_book_user)]
for row in book_df_rows.itertuples():
    print(f"Judul     : {row.book_title}")
    print(f"Penulis   : {row.book_author}")
    print(f"Penerbit  : {row.publisher}")
    print("----" * 8)

print('====' * 8)
print('Top 10 book recommendation')
print('====' * 8)

recommended_book = book_df[book_df['id'].isin(recommended_book_ids)]
for row in recommended_book.itertuples():
    print(f"Judul     : {row.book_title}")
    print(f"Penulis   : {row.book_author}")
    print(f"Penerbit  : {row.publisher}")
    print("----" * 8)

"""# Kesimpulan
Dalam proyek ini, sistem rekomendasi buku dibangun untuk membantu pengguna menemukan buku yang sesuai dengan minat mereka di tengah banyaknya pilihan yang tersedia. Melalui dua pendekatan, yaitu content-based filtering dan collaborative filtering berbasis deep learning, sistem mampu memberikan rekomendasi yang akurat dan relevan.

Model collaborative filtering menunjukkan performa sangat baik dengan nilai RMSE rendah (0.1991) dan Precision@10 serta Recall@10 yang tinggi (0.8389 dan 0.6089), yang menunjukkan bahwa sistem dapat memberikan rekomendasi personal yang efektif dan efisien.

Sementara itu, content-based filtering berhasil menghasilkan rekomendasi yang konsisten dan relevan berdasarkan fitur konten buku, seperti judul dan nama penulis. Dengan precision 1.0, sistem mampu memberikan hasil yang cukup relevan tanpa bergantung pada data pengguna lain.

Secara keseluruhan, kedua pendekatan saling melengkapi dan dapat digunakan dalam kombinasi (hybrid) untuk meningkatkan kualitas sistem rekomendasi buku secara keseluruhan.


"""